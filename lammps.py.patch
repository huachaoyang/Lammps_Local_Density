1c1
< #/usr/bin/env python
---
>  #/usr/bin/env python
6a7
> import os
17a19,21
> #parallel lammps
> RunParallel = True
> NSlaves = 4
785a790,793
>     if RunParallel: 
>         JobFile = os.path.join(os.path.dirname(LammpsFiles[0]), 'lmp_MD.sh')
>     else: JobFile = ''     
>     LammpsFiles.append(JobFile)
813c821
< def RunLammps(InFile, Prefix = "", LogFile = "lammps.log", Verbose = False,
---
> def RunLammps(InFile, JobFile = 'lmp_MD.sh', Prefix = "", LogFile = "lammps.log", Verbose = False,
822a831
> 
824,826c833,847
<     p = subprocess.Popen(s, shell = True, 
<                          stdout = subprocess.PIPE,
<                          stderr = subprocess.STDOUT)
---
>     # freecores - built by SPC and kept in /home/tsanyal/.scripts
>     NSlaves_MAX = int(os.popen('freecores').read().strip())
>     if RunParallel: # and NSlaves <= NSlaves_MAX:
>     	print 'Submitting parallel job on %d cores' % NSlaves    
>         #p = RunParLammps(InFile = InFile, LogFile = LogFile, JobFile = JobFile, NSlaves = NSlaves)
>         s = "mpiexec -np %d " % NSlaves + s
>         p = subprocess.Popen(s, shell = True, 
>                              stdout = subprocess.PIPE,
>                              stderr = subprocess.STDOUT)
>     else:
>     	print 'Serial job run from currently active core'
>     	p = subprocess.Popen(s, shell = True, 
>                              stdout = subprocess.PIPE,
>                              stderr = subprocess.STDOUT)
>     
854c875,876
<     LogFile, returncode = RunLammps(InFile, Prefix = Prefix, Verbose = Verbose)
---
>     if RunParallel: JobFile = LammpsFiles[-1]
>     LogFile, returncode = RunLammps(InFile, JobFile = JobFile, Prefix = Prefix, Verbose = Verbose)
857c879
<         for fn in LammpsFiles + [LogFile]:
---
>         for fn in LammpsFiles + [LogFile] + [JobFile]:
863c885,934
<     
\ No newline at end of file
---
> def RunParLammps(InFile, LogFile, JobFile, NSlaves):
> 	"""runs a parallel lammps job if there
> are sufficient cores available
> 	"""
> 	
> 	## Keep track of current directory
> 	## If jobscript is in a different dir, might need to change to that
> 	## before submitting
> 	srel_dir = os.getcwd()
> 	JobDir = os.path.dirname(InFile)
> 	
> 	## Parallel MPI jobscript specific to Sun-Grid-Engine (zin)
> 	s = """
> #!/bin/bash
> #
> #$ -cwd
> #$ -V
> #$ -j y
> #$ -S /bin/bash
> #$ -N %(JobName)s
> #$ -pe ompi %(NSlaves)d
> 
> date
> mpiexec -np %(NSlaves)d %(LammpsExec)s -in %(InFile)s -log %(LogFile)s
> 	"""
> 	
> 	import subprocess
> 	print InFile, LogFile, JobFile
> 	d = {'JobName': 'lammps_MD_%dcore' % NSlaves, 'NSlaves' : NSlaves, 
>           'LammpsExec': LammpsExec.strip(), 'InFile': InFile, 'LogFile': LogFile}
> 	
> 	file(JobFile, 'w').write(s % d)
> 	
> 	## Bash script to wait till job ends specific to Sun-Grid-Engine (zin)
> 	waitscript = """
> chmod 777 %(JobFile)s
> echo "Holding execution till job returns..."
> cd %(JobDir)s
> #TODO: submit Job from head node through ssh
> JOBID=$(ssh zin "cd `pwd` ; echo $($SGE_ROOT/$SGE_CELLqsub %(JobFile)s) | awk '{print $3}')
> while qstat -u $USER | grep $JOBID &> /dev/null; do
>     sleep 5;
> done;
> """ 
> 	waitscript_dict = {'JobFile': JobFile, 'JobDir': JobDir}
> 	p = subprocess.Popen(waitscript % waitscript_dict, shell = True)
> 	os.chdir(srel_dir)
> 	return p
> 
> 	   
